# folding_rnn.yaml

model:
  class_path: lib.training.modules.PipelineModule
  init_args:
    model:
      class_path: lib.models.recurrent.RecurrentEncoderDecoderWithAttention
      init_args:
        num_embeddings: 8
        embedding_dim: 128
        hidden_size: 128
        out_size: 3
        num_encoder_layers: 6
        num_decoder_layers: 6
        num_heads: 8
        num_concat_dims: 2
    objectives:
      loss:
        class_path: loss.KabschLoss

data:
  class_path: lib.data.datamodules.netCDFDataModule
  init_args:
    paths:
      train:
        - data/train
      validate:
        - data/val
      predict:
        - data/train
        - data/val
    batch_size: 4
    input_variables:
      - [sequencechain, x]
    target_variables:
      - [coordinates, y]

trainer:
  gradient_clip_val: 1.0
  max_epochs: 50
  accumulate_grad_batches: 16
  devices: 1
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      save_dir: ./logs/
      log_model: all
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        filename: best_model
        mode: min
        save_top_k: 1
        save_last: true
        dirpath: ./checkpoints/
    - class_path: lib.data.writing.netCDFDistributedPredictionWriter
      init_args:
        path: data/predictions
        variable_name: coordinates
        dimension_names: 
          - batch
          - residue
          - xyz
        write_interval: batch

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0005